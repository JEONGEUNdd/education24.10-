{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7e067ec-14b7-431b-a221-21b6c6ea8681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "페이지제목: NAVER\n",
      "검색 결과 페이지 제목: 파이썬 : 네이버 검색\n"
     ]
    }
   ],
   "source": [
    "#네이버에서 '파이썬'검색\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "options=webdriver.ChromeOptions()\n",
    "#options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "time.sleep(5)\n",
    "driver.get(\"https://www.naver.com\")\n",
    "print(\"페이지제목:\", driver.title)\n",
    "\n",
    "#검색창찾기\n",
    "search_box=driver.find_element(By.NAME, 'query')\n",
    "search_box.send_keys('파이썬')\n",
    "search_box.submit()\n",
    "\n",
    "print(\"검색 결과 페이지 제목:\", driver.title)\n",
    "\n",
    "time.sleep(30)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea221e97-b96b-4b2a-936b-bb34a5a9c991",
   "metadata": {},
   "source": [
    "### 스크래핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b473f857-64ee-4196-b383-91469f21beea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[속보] 검찰, '명태균 의혹' 관련 창원시 압수수색\",\n",
       " \"국민연금 2천만원이상 받아좋지만…건보 피부양자 탈락에 '울상'\",\n",
       " \"온라인 쇼핑서 월평균 11만원 결제한다…품질불량 '최다 불만'\",\n",
       " '비탈길 주차 화물차 미끄러져 노인 덮쳐…운전자는 무죄',\n",
       " '인천 동춘동 아파트서 불… 주민 2명 다쳐',\n",
       " '부천 상동 홈플러스 공공기여 ‘고래 싸움에 새우등’…고통받는 시행사',\n",
       " 'FC안양 수의계약 ‘논란’…법적인 절차 위반·부실 감사 의혹 제기']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        self.options=webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--headless')\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    def create_driver(self):\n",
    "        return webdriver.Chrome(options=self.options)\n",
    "\n",
    "    def scape_naver_headlines(self):\n",
    "        driver = self.create_driver()\n",
    "        try:\n",
    "            driver.get(\"https://news.naver.com\")\n",
    "            items = driver.find_elements(By.CSS_SELECTOR,'a._editn_link')[:7]\n",
    "            return [item.text.strip() for item in items]\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "scp = WebScraper()\n",
    "dt1 = scp.scape_naver_headlines()\n",
    "dt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bcd0fda7-b098-40a7-bdd6-619c1a2c9ee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'A Light in the Attic', 'price': '£51.77', 'score': 3},\n",
       " {'title': 'Tipping the Velvet', 'price': '£53.74', 'score': 1},\n",
       " {'title': 'Soumission', 'price': '£50.10', 'score': 1},\n",
       " {'title': 'Sharp Objects', 'price': '£47.82', 'score': 4},\n",
       " {'title': 'Sapiens: A Brief History of Humankind',\n",
       "  'price': '£54.23',\n",
       "  'score': 5},\n",
       " {'title': 'The Requiem Red', 'price': '£22.65', 'score': 1},\n",
       " {'title': 'The Dirty Little Secrets of Getting Your Dream Job',\n",
       "  'price': '£33.34',\n",
       "  'score': 4},\n",
       " {'title': 'The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull',\n",
       "  'price': '£17.93',\n",
       "  'score': 3},\n",
       " {'title': 'The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics',\n",
       "  'price': '£22.60',\n",
       "  'score': 4},\n",
       " {'title': 'The Black Maria', 'price': '£52.15', 'score': 1},\n",
       " {'title': 'Starving Hearts (Triangular Trade Trilogy, #1)',\n",
       "  'price': '£13.99',\n",
       "  'score': 2},\n",
       " {'title': \"Shakespeare's Sonnets\", 'price': '£20.66', 'score': 4},\n",
       " {'title': 'Set Me Free', 'price': '£17.46', 'score': 5},\n",
       " {'title': \"Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\",\n",
       "  'price': '£52.29',\n",
       "  'score': 5},\n",
       " {'title': 'Rip it Up and Start Again', 'price': '£35.02', 'score': 5},\n",
       " {'title': 'Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991',\n",
       "  'price': '£57.25',\n",
       "  'score': 3},\n",
       " {'title': 'Olio', 'price': '£23.88', 'score': 1},\n",
       " {'title': 'Mesaerion: The Best Science Fiction Stories 1800-1849',\n",
       "  'price': '£37.59',\n",
       "  'score': 1},\n",
       " {'title': 'Libertarianism for Beginners', 'price': '£51.33', 'score': 2},\n",
       " {'title': \"It's Only the Himalayas\", 'price': '£45.17', 'score': 2}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ebook 스크래핑\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        self.options=webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--headless')\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    def create_driver(self):\n",
    "        return webdriver.Chrome(options=self.options)\n",
    "\n",
    "    def scape_ebook(self):\n",
    "        driver = self.create_driver()\n",
    "        # rt={\"One\":1, \"Two\":2, \"Three\":3, \"Four\":4, \"Five\":5}\n",
    "        posts = [] \n",
    "        try:\n",
    "            driver.get(\"https://books.toscrape.com/\")\n",
    "            items = driver.find_elements(By.CSS_SELECTOR,'.product_pod')\n",
    "            for item in items:\n",
    "                title = item.find_element(By.CSS_SELECTOR, 'h3 a').get_attribute('title')\n",
    "                price = item.find_element(By.CSS_SELECTOR, '.price_color').text.strip()\n",
    "                score = item.find_element(By.CSS_SELECTOR, '.star-rating').get_attribute('class').split()[-1]  #공백으로도 나눔\n",
    "                # score = rt[item.find_element(By.CSS_SELECTOR, '.star-rating').get_attribute('class').split()[-1]]\n",
    "                if score== \"One\":\n",
    "                    score = 1\n",
    "                elif score == \"Two\":\n",
    "                    score = 2\n",
    "                elif score == \"Three\":\n",
    "                    score = 3\n",
    "                elif score == \"Four\":\n",
    "                    score = 4\n",
    "                elif score == \"Five\":\n",
    "                    score = 5\n",
    "                else:\n",
    "                    score = 0 \n",
    "                    \n",
    "                posts.append({\n",
    "                    'title': title,\n",
    "                    'price': price,\n",
    "                    'score': score\n",
    "                })\n",
    "        finally:\n",
    "            driver.quit()\n",
    "        return posts  \n",
    "        \n",
    "scp = WebScraper()\n",
    "dt1 = scp.scape_ebook()\n",
    "dt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "40aaad33-ed3e-49c8-877f-6d06c89df594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temperature': '현재 온도 0.6°', 'weather': '맑음'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#날씨 \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        self.options=webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--headless')\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    def create_driver(self):\n",
    "        return webdriver.Chrome(options=self.options)\n",
    "        \n",
    "    def scape_weather(self):\n",
    "        driver = self.create_driver()\n",
    "\n",
    "        try:\n",
    "            driver.get(\"https://weather.naver.com/\")\n",
    "            WebDriverWait(driver,10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'strong.current'))\n",
    "            )\n",
    "            \n",
    "            temp = driver.find_element(By.CSS_SELECTOR,'strong.current').text.replace('\\n',' ')\n",
    "            weather=driver.find_element(By.CSS_SELECTOR,'.summary .weather').text\n",
    "            return {'temperature':temp, 'weather':weather}\n",
    "        finally:\n",
    "            driver.quit()                   \n",
    "            \n",
    "scp = WebScraper()\n",
    "dt1 = scp.scape_weather()\n",
    "dt1         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "753af7a7-2138-475f-9383-d8792afca4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'about': '파이썬 코딩의 기술 개정2판(Effective Python, 2nd) 소스 코드입니다',\n",
       " 'reponame': '080235',\n",
       " 'fork': '41',\n",
       " 'star': '63'}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#깃허브\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        self.options=webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--headless')\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    def create_driver(self):\n",
    "        return webdriver.Chrome(options=self.options)\n",
    "        \n",
    "    def scape_github(self):\n",
    "        driver = self.create_driver()\n",
    "\n",
    "        try:\n",
    "            driver.get(\"https://github.com/gilbutiTbook/080235\")\n",
    "            \n",
    "            about = driver.find_element(By.CSS_SELECTOR,'p.my-3').text.strip() \n",
    "            reponame=driver.find_element(By.CSS_SELECTOR,'strong.mr-2 a').text.strip()\n",
    "            fork=driver.find_element(By.CSS_SELECTOR,'#repo-network-counter').text.strip()\n",
    "            star=driver.find_element(By.CSS_SELECTOR,'#repo-stars-counter-star').text.strip()\n",
    "            return {'about':about, 'reponame':reponame, 'fork':fork,'star':star}\n",
    "        finally:\n",
    "            driver.quit()                   \n",
    "            \n",
    "scp = WebScraper()\n",
    "dt1 = scp.scape_github()\n",
    "dt1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5179074a-7029-4430-8e86-a8c5d4bb32bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '삼성전자', 'code': '005930', 'price': '54,200'}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#삼성전자 주식\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        self.options=webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--headless')\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    def create_driver(self):\n",
    "        return webdriver.Chrome(options=self.options)\n",
    "        \n",
    "    def scape_stock(self,stock_code):\n",
    "        driver = self.create_driver()\n",
    "\n",
    "        try:\n",
    "            driver.get(\"https://finance.naver.com/item/main.naver?code=005930\")\n",
    "            name=driver.find_element(By.CSS_SELECTOR,'.wrap_company a').text.strip() \n",
    "            code=driver.find_element(By.CSS_SELECTOR,'span.code').text.strip() \n",
    "            price = driver.find_element(By.CSS_SELECTOR,'em.no_down').text.replace('\\n','')\n",
    "            \n",
    "            return {'name':name, 'code':code,'price':price}\n",
    "        finally:\n",
    "            driver.quit()                   \n",
    "            \n",
    "scp = WebScraper()\n",
    "dt1 = scp.scape_stock('005930')\n",
    "dt1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "b2bbf96e-4729-46d1-a6e2-957b615df80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '삼성전자', 'code': '005930코스피', 'price': 54200}"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#삼성전자 주식\n",
    "#반응형웹\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        self.options=webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--headless')\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    def create_driver(self):\n",
    "        return webdriver.Chrome(options=self.options)\n",
    "        \n",
    "    def scape_stock(self,stock_code):\n",
    "        driver = self.create_driver()\n",
    "\n",
    "        try:\n",
    "            driver.get(\"https://m.stock.naver.com/domestic/stock/005930/total\")\n",
    "            name=driver.find_element(By.CSS_SELECTOR,'span.GraphMain_name__r5bQX').text.strip() \n",
    "            code=driver.find_element(By.CSS_SELECTOR,'span.GraphMain_code__1MqvF').text.strip() \n",
    "            price = int(driver.find_element(By.CSS_SELECTOR,'.GraphMain_price__GT8dV').text.strip().replace('\\n원','').replace(',',''))\n",
    "            \n",
    "            return {'name':name, 'code':code,'price':price}\n",
    "        finally:\n",
    "            driver.quit()                   \n",
    "            \n",
    "scp = WebScraper()\n",
    "dt1 = scp.scape_stock('005930')\n",
    "dt1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "59c67d86-89a2-47aa-8a70-8d0753678c0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어를 입력하세요:  파이썬\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 완료\n",
      "2페이지 완료\n",
      "\n",
      "===검색 결과===\n",
      "검색어: 파이썬\n",
      "수집 시간: 2024-11-29 16:28:36\n",
      "총 20개의 기사 수집\n",
      "\n",
      "1.AI 교육혁명시대, 새로운 대학교육의 리더 서울사이버대 12월1일부터 모집\n",
      "   링크: https://www.hani.co.kr/arti/society/schooling/1169651.html\n",
      "\n",
      "2.양자컴퓨터 국내 첫 도입한 연세대…IBM과 불치병 잡을까\n",
      "   링크: https://zdnet.co.kr/view/?no=20241120145052\n",
      "\n",
      "3.‘애저 AI 파운드리’ 생성형 AI 개발의 도약적 출발선\n",
      "   링크: https://byline.network/?p=9004111222549630\n",
      "\n",
      "4.최근5년 신입생 수 1위… ‘인공지능 휴먼’ 강의 제작\n",
      "   링크: https://www.munhwa.com/news/view.html?no=2024112601032621000002\n",
      "\n",
      "5.이파피루스, '파이뮤PDF 프로' 국내 출시\n",
      "   링크: https://www.etnews.com/20241120000073\n",
      "\n",
      "6.코딩을 다시 즐겁게··· 개발자를 위한 14가지 전처리기\n",
      "   링크: https://www.cio.com/article/3611726/코딩을-다시-즐겁게···-개발자를-위한-14가지-전처리.html\n",
      "\n",
      "7.구디아카데미, 국비지원 자바 부트캠프 'AI 부트캠프' 12월 개강\n",
      "   링크: https://edu.donga.com/news/articleView.html?idxno=78661\n",
      "\n",
      "8.코딩밸리, 신규 ‘파이썬 업무 자동화 코스’ 출시\n",
      "   링크: https://magazine.hankyung.com/business/article/202411089559b\n",
      "\n",
      "9.반에크 유럽 파이썬 ETN 출시, 세계 15개국 투자자 접근 가능\n",
      "   링크: https://www.tokenpost.kr/article-204120\n",
      "\n",
      "10.파이썬, AI 열풍 힘입어 깃허브 최고 인기 언어 등극\n",
      "   링크: https://zdnet.co.kr/view/?no=20241104100443\n",
      "\n",
      "11.이파피루스, 문서 데이터 추출하는 '파이뮤PDF 프로' 국내 정식 론칭\n",
      "   링크: http://www.itdaily.kr/news/articleView.html?idxno=228903\n",
      "\n",
      "12.중앙대광명병원, 제2회 스마트챌린지 개최… 디지털 전환 고도화 추진\n",
      "   링크: https://health.chosun.com/site/data/html_dir/2024/11/22/2024112201190.html\n",
      "\n",
      "13.한국항공대, '빅데이터 분석을 위한 파이썬 입문 과정' 실시\n",
      "   링크: https://news.unn.net/news/articleView.html?idxno=570505\n",
      "\n",
      "14.윤가이, ‘코딩밸리’ 서비스 전속 모델 발탁..CF 촬영 진행\n",
      "   링크: https://www.mk.co.kr/article/11175418?refer=portal\n",
      "\n",
      "15.코딩밸리, 신규 ‘파이썬 업무 자동화 코스’ 출시\n",
      "   링크: https://magazine.hankyung.com/business/article/202411089559b\n",
      "\n",
      "16.엔비디아, cu파이뉴메릭 가속 컴퓨팅 라이브러리 공개\n",
      "   링크: https://kbench.com/?q=node/262160\n",
      "\n",
      "17.반에크 유럽 파이썬 ETN 출시, 세계 15개국 투자자 접근 가능\n",
      "   링크: https://www.tokenpost.kr/article-204120\n",
      "\n",
      "18.가천대, ‘With 청년, 취업 Catch Up’ 프로그램으로 지역 청년 지원 강화\n",
      "   링크: https://www.kgnews.co.kr/news/article.html?no=817577\n",
      "\n",
      "19.가천대, 청년 취업 지원 거점형 프로그램 운영\n",
      "   링크: http://www.enewstoday.co.kr/news/articleView.html?idxno=2200052\n",
      "\n",
      "20.코딩학원 코리아IT아카데미, 최후의 1인 가리는 ‘코지컬 100’ 코딩대회 개최\n",
      "   링크: http://www.segye.com/newsView/20241108509213?OutUrl=naver\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[261], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m keyword\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m검색어를 입력하세요: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m num_pages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 46\u001b[0m get_news_titles(keyword,num_pages)\n",
      "Cell \u001b[1;32mIn[261], line 41\u001b[0m, in \u001b[0;36mget_news_titles\u001b[1;34m(keyword, num_pages)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   링크: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m---> 41\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(main_titles)\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnavernews.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m driver\u001b[38;5;241m.\u001b[39mquit()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:867\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    859\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    860\u001b[0m             arrays,\n\u001b[0;32m    861\u001b[0m             columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 867\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    868\u001b[0m             data,\n\u001b[0;32m    869\u001b[0m             index,\n\u001b[0;32m    870\u001b[0m             columns,\n\u001b[0;32m    871\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    872\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    873\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    874\u001b[0m         )\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    876\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    877\u001b[0m         {},\n\u001b[0;32m    878\u001b[0m         index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    882\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:319\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    314\u001b[0m     values \u001b[38;5;241m=\u001b[39m _ensure_2d(values)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m     values \u001b[38;5;241m=\u001b[39m _prep_ndarraylike(values, copy\u001b[38;5;241m=\u001b[39mcopy_on_sanitize)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m values\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;66;03m# GH#40110 see similar check inside sanitize_array\u001b[39;00m\n\u001b[0;32m    323\u001b[0m     values \u001b[38;5;241m=\u001b[39m sanitize_array(\n\u001b[0;32m    324\u001b[0m         values,\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    328\u001b[0m         allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:575\u001b[0m, in \u001b[0;36m_prep_ndarraylike\u001b[1;34m(values, copy)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# we could have a 1-dim or 2-dim list here\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;66;03m# this is equiv of np.asarray, but does object conversion\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;66;03m# and platform dtype preservation\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;66;03m# does not convert e.g. [1, \"a\", True] to [\"1\", \"a\", \"True\"] like\u001b[39;00m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;66;03m#  np.asarray would\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(values[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m--> 575\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([convert(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values[\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m values[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;66;03m# GH#21861 see test_constructor_list_of_lists\u001b[39;00m\n\u001b[0;32m    578\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([convert(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "#네이버뉴스에서 키워드 검색해 기사제목, 링크 추출\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def get_news_titles(keyword,num_pages):\n",
    "    driver=webdriver.Chrome()\n",
    "    titles=[]\n",
    "    \n",
    "    for page in range(1,num_pages+1):\n",
    "        url=f\"https://search.naver.com/search.naver?where=news&query={keyword}&start={(page-1)*10+1}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        soup=BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        main_titles=soup.select(\"a.news_tit\")\n",
    "\n",
    "        for title in main_titles:\n",
    "            titles.append({\n",
    "                'title':title.get('title'),\n",
    "                'link':title.get('href')\n",
    "                \n",
    "            })\n",
    "        print(f\"{page}페이지 완료\")\n",
    "\n",
    "    print(\"\\n===검색 결과===\")\n",
    "    print(f\"검색어: {keyword}\")\n",
    "    print(f\"수집 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"총 {len(titles)}개의 기사 수집\\n\")\n",
    "\n",
    "    for idx,item in enumerate(titles,1):\n",
    "        print(f\"{idx}.{item['title']}\")\n",
    "        print(f\"   링크: {item['link']}\")\n",
    "        print()\n",
    "    \n",
    "    pd.DataFrame(titles).to_csv('navernews.csv',index=False,encoding='utf-8')\n",
    "    driver.quit()\n",
    "    \n",
    "keyword=input(\"검색어를 입력하세요: \")\n",
    "num_pages=2\n",
    "get_news_titles(keyword,num_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "358b905d-7bf1-4a0b-8f6d-195e6aa07fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색할 키워드를 입력하세요:  파이썬\n",
      "수집할 페이지 수를 입력하세요:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'파이썬' 관련 뉴스 수집을 시작합니다...\n",
      "1페이지 완료\n",
      "2페이지 완료\n",
      "\n",
      "=== 저장 완료 ===\n",
      "파일 저장 위치: news_data/naver_news_파이썬_20241129_163426.csv\n",
      "총 기사 수: 20개\n",
      "\n",
      "=== 데이터 통계 ===\n",
      "언론사별 기사 수:\n",
      "언론사\n",
      "한경비즈니스    2\n",
      "토큰포스트     2\n",
      "한겨레       1\n",
      "아이티데일리    1\n",
      "이뉴스투데이    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== 작업 완료 ===\n",
      "CSV 파일: news_data/naver_news_파이썬_20241129_163426.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class NaverNewsCollector:\n",
    "    def __init__(self):\n",
    "        self.driver = webdriver.Chrome()\n",
    "        \n",
    "    def collect_news(self, keyword, num_pages):\n",
    "        articles = []\n",
    "        \n",
    "        try:\n",
    "            for page in range(1, num_pages + 1):\n",
    "                url = f\"https://search.naver.com/search.naver?where=news&query={keyword}&start={(page-1)*10 + 1}\"\n",
    "                self.driver.get(url)\n",
    "                time.sleep(2)\n",
    "                \n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                news_items = soup.select(\"div.news_wrap.api_ani_send\")\n",
    "                \n",
    "                for item in news_items:\n",
    "                    try:\n",
    "                        title_element = item.select_one(\"a.news_tit\")\n",
    "                        if not title_element:\n",
    "                            continue\n",
    "                            \n",
    "                        title = title_element['title']\n",
    "                        link = title_element['href']\n",
    "                        \n",
    "                        press = item.select_one(\"a.press\")\n",
    "                        press_name = press.text if press else \"알 수 없음\"\n",
    "                        \n",
    "                        date = item.select_one(\"span.info\")\n",
    "                        date_text = date.text if date else \"날짜 없음\"\n",
    "                        \n",
    "                        articles.append({\n",
    "                            '수집일시': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                            '검색어': keyword,\n",
    "                            '제목': title,\n",
    "                            '언론사': press_name,\n",
    "                            '보도일자': date_text,\n",
    "                            '링크': link\n",
    "                        })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"기사 추출 중 에러: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                print(f\"{page}페이지 완료\")\n",
    "                \n",
    "            return articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"에러 발생: {e}\")\n",
    "            return articles\n",
    "\n",
    "    def save_to_csv(self, articles, keyword):\n",
    "        try:\n",
    "            df = pd.DataFrame(articles)\n",
    "            \n",
    "            output_dir = 'news_data'\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            \n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f\"{output_dir}/naver_news_{keyword}_{timestamp}.csv\"\n",
    "            \n",
    "            df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            print(\"\\n=== 저장 완료 ===\")\n",
    "            print(f\"파일 저장 위치: {filename}\")\n",
    "            print(f\"총 기사 수: {len(articles)}개\")\n",
    "            print(\"\\n=== 데이터 통계 ===\")\n",
    "            print(\"언론사별 기사 수:\")\n",
    "            print(df['언론사'].value_counts().head())\n",
    "            \n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"CSV 저장 중 에러 발생: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "def main():\n",
    "    keyword = input(\"검색할 키워드를 입력하세요: \")\n",
    "    num_pages = int(input(\"수집할 페이지 수를 입력하세요: \"))\n",
    "    \n",
    "    collector = NaverNewsCollector()\n",
    "    try:\n",
    "        print(f\"'{keyword}' 관련 뉴스 수집을 시작합니다...\")\n",
    "        articles = collector.collect_news(keyword, num_pages)\n",
    "        \n",
    "        if articles:\n",
    "            csv_file = collector.save_to_csv(articles, keyword)\n",
    "            print(\"\\n=== 작업 완료 ===\")\n",
    "            if csv_file:\n",
    "                print(f\"CSV 파일: {csv_file}\")\n",
    "           \n",
    "    finally:\n",
    "        collector.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a86477e3-8cf2-4e47-b00b-822be96ed1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스크래핑이 완료되었습니다. 'books_to_scrape.html' 파일과 이미지를 확인하세요.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# 기본 URL\n",
    "BASE_URL = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
    "IMAGE_BASE_URL = \"https://books.toscrape.com/\"\n",
    "\n",
    "# 책 데이터 저장\n",
    "book_data = []\n",
    "\n",
    "# 이미지 저장 디렉토리\n",
    "IMAGE_DIR = \"book_images\"\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "# 페이지 순회\n",
    "for page in range(1, 51):  # 이 사이트는 50페이지로 고정되어 있음\n",
    "    url = BASE_URL.format(page)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"페이지 {page}를 가져올 수 없습니다.\")\n",
    "        break\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    books = soup.find_all('article', class_='product_pod')\n",
    "    \n",
    "    for book in books:\n",
    "        title = book.h3.a['title']\n",
    "        price = book.find('p', class_='price_color').text\n",
    "        availability = book.find('p', class_='instock availability').text.strip()\n",
    "        image_url = IMAGE_BASE_URL + book.find('img')['src']\n",
    "        rating_class = book.find('p', class_='star-rating')['class'][1]  # 평점 클래스\n",
    "        \n",
    "        # 이미지 다운로드\n",
    "        image_response = requests.get(image_url)\n",
    "        image_filename = os.path.join(IMAGE_DIR, image_url.split('/')[-1])\n",
    "        with open(image_filename, 'wb') as img_file:\n",
    "            img_file.write(image_response.content)\n",
    "        \n",
    "        book_data.append({\n",
    "            'title': title,\n",
    "            'price': price,\n",
    "            'availability': availability,\n",
    "            'image': image_filename,\n",
    "            'rating': rating_class\n",
    "        })\n",
    "\n",
    "# HTML 파일로 저장\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "    <title>Books to Scrape</title>\n",
    "    <style>\n",
    "        body { font-family: Arial, sans-serif; line-height: 1.6; }\n",
    "        .book { display: flex; align-items: center; border: 1px solid #ddd; margin: 10px; padding: 10px; }\n",
    "        .book img { max-width: 100px; margin-right: 20px; }\n",
    "        .book-info { flex: 1; }\n",
    "        .rating { color: gold; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "<h1>Books to Scrape</h1>\n",
    "\"\"\"\n",
    "\n",
    "for book in book_data:\n",
    "    # 평점 별 표현 (별 개수로 표시)\n",
    "    rating_stars = \"★\" * [\"Zero\", \"One\", \"Two\", \"Three\", \"Four\", \"Five\"].index(book['rating'])\n",
    "    html_content += f\"\"\"\n",
    "    <div class=\"book\">\n",
    "        <img src=\"{book['image']}\" alt=\"{book['title']}\">\n",
    "        <div class=\"book-info\">\n",
    "            <h2>{book['title']}</h2>\n",
    "            <p><strong>Price:</strong> {book['price']}</p>\n",
    "            <p><strong>Availability:</strong> {book['availability']}</p>\n",
    "            <p class=\"rating\"><strong>Rating:</strong> {rating_stars}</p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "html_content += \"\"\"\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "with open(\"books_to_scrape1.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(\"스크래핑이 완료되었습니다. 'books_to_scrape.html' 파일과 이미지를 확인하세요.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
